<!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->

<div align="center">
<h1>
  Baichuan-13B
</h1>
</div>

<p align="center">
ğŸ¤— <a href="https://huggingface.co/baichuan-inc/Baichuan-13B-Base" target="_blank">Baichuan-13B-Base</a> 
  â€¢ 
ğŸ¤— <a href="https://huggingface.co/baichuan-inc/Baichuan-13B-Chat" target="_blank">Baichuan-13B-Chat</a> 
  â€¢ 
ğŸ¤– <a href="https://modelscope.cn/organization/baichuan-inc" target="_blank">ModelScope</a> 
  â€¢ 
ğŸ’¬ <a href="https://github.com/baichuan-inc/Baichuan-13B/blob/main/media/wechat.jpeg?raw=true" target="_blank">WeChat</a>
</p>

<div align="center">

[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/Baichuan-inc/baichuan-13B/blob/main/LICENSE)
<h4 align="center">
    <p>
        <b>English</b> |
        <a href="https://github.com/baichuan-inc/Baichuan-13B/blob/main/README.md">ä¸­æ–‡</a>
    <p>
</h4>
</div>

# Update
[2023.08.01] Updated weights of the aligned model [Baichuan-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat), optimizing the effects in some scenarios.

# Table of Contents

- [Introduction](#Introduction)
- [Benchmark Results](#Benchmark-Results)
- [Model Details](#Model-Details)
- [Inference and Deployment](#Inference-and-Deployment)
- [Fine-tuning](#Fine-tuning)
- [Disclaimer](#Disclaimer)
- [Licenses](#Licenses)

# Introduction

Baichuan-13B is an open-source, commercially available large-scale language model developed by Baichuan Intelligent Technology following [Baichuan-7B](https://github.com/baichuan-inc/baichuan-7B), containing 13 billion parameters. It achieves the best results of the same size on both authoritative Chinese and English benchmarks. This release includes two versions: pre-training ([Baichuan-13B-Base](https://huggingface.co/baichuan-inc/Baichuan-13B-Base)) and alignment ([Baichuan-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat)). Baichuan-13B has the following features:

  1. **Larger size, more data**: Baichuan-13B further expands the number of parameters to 13 billion based on [Baichuan-7B](https://github.com/baichuan-inc/baichuan-7B), and has trained 1.4 trillion tokens on high-quality corpora, exceeding LLaMA-13B by 40%. It is currently the model with the most training data under the open source 13B size. It supports both Chinese and English, uses ALiBi positional encoding, and has a context window length of 4096.
  2. **Pre-training and alignment models**: The pre-training model is a "base" suitable for developers, while the general users have a stronger demand for the aligned model with dialogue functions. Therefore, this open-source release also includes the aligned model (Baichuan-13B-Chat), which has strong dialogue capabilities, is ready-to-use, and can be simply deployed with a few lines of code.
  3. **More efficient inference**: To support wider user use, we are also open int8 and int4 quantized versions. Compared with the non-quantized version, they greatly lower the machine resource threshold for deployment with almost no loss of performance, and can be deployed on consumer-grade graphics cards like the Nvidia 3090.
  4. **Open source, free and available for commercial use**: Baichuan-13B is not only fully open to academic research, but developers can also use it commercially for free, just by applying for and obtaining an official commercial license via email.

# Benchmark Results

We conducted a `5-shot` evaluation on various authoritative Chinese and English benchmarks. The results are as follows:

## [C-Eval](https://cevalbenchmark.com/index.html#home)

| Model 5-shot            | STEM  | Social Sciences | Humanities | Others | Average |
|-------------------------|:-----:|:---------------:|:----------:|:------:|:-------:|
| Baichuan-7B             | 38.2  | 52.0            | 46.2       | 39.3   | 42.8    |
| Chinese-Alpaca-Plus-13B | 35.2  | 45.6            | 40.0       | 38.2   | 38.8    |
| Vicuna-13B              | 30.5  | 38.2            | 32.5       | 32.5   | 32.8    |
| Chinese-LLaMA-Plus-13B  | 30.3  | 38.0            | 32.9       | 29.1   | 32.1    |
| Ziya-LLaMA-13B-Pretrain | 27.6  | 34.4            | 32.0       | 28.6   | 30.0    |
| LLaMA-13B               | 27.0  | 33.6            | 27.7       | 27.6   | 28.5    |
| moss-moon-003-base (16B)| 27.0  | 29.1            | 27.2       | 26.9   | 27.4    |
| **Baichuan-13B-Base**   | **45.9** | **63.5** | **57.2**    | **49.3** | **52.4** |
| **Baichuan-13B-Chat**   | **43.7** | **64.6** | **56.2**    | **49.2** | **51.5** |


## [MMLU](https://arxiv.org/abs/2009.03300)

| Model 5-shot            | STEM  | Social Sciences | Humanities | Others | Average |
|-------------------------|:-----:|:---------------:|:----------:|:------:|:-------:|
| Vicuna-13B              | 40.4  | 60.5            | 49.5       | 58.4   | 52.0    |
| LLaMA-13B               | 36.1  | 53.0            | 44.0       | 52.8   | 46.3    |
| Chinese-Alpaca-Plus-13B | 36.9  | 48.9            | 40.5       | 50.5   | 43.9    |
| Ziya-LLaMA-13B-Pretrain | 35.6  | 47.6            | 40.1       | 49.4   | 42.9    |
| Baichuan-7B             | 35.6  | 48.9            | 38.4       | 48.1   | 42.3    |
| Chinese-LLaMA-Plus-13B  | 33.1  | 42.8            | 37.0       | 44.6   | 39.2    |
| moss-moon-003-base (16B)| 22.4  | 22.8            | 24.2       | 24.4   | 23.6    |
| **Baichuan-13B-Base**   | **41.6** | **60.9** | **47.4**    | **58.5** | **51.6** |
| **Baichuan-13B-Chat**   | **40.9** | **60.9** | **48.8**    | **59.0** | **52.1** |
> Note: We adopted the offical [Evaluation Scheme](https://github.com/hendrycks/test) from MMLU.

## [CMMLU](https://github.com/haonan-li/CMMLU)

| Model 5-shot            | STEM  | Humanities | Social Sciences | Others | China Specific | Average |
|-------------------------|:-----:|:----------:|:---------------:|:------:|:--------------:|:-------:|
| Baichuan-7B             | 34.4  | 47.5       | 47.6            | 46.6   | 44.3           | 44.0    |
| Vicuna-13B              | 31.8  | 36.2       | 37.6            | 39.5   | 34.3           | 36.3    |
| Chinese-Alpaca-Plus-13B | 29.8  | 33.4       | 33.2            | 37.9   | 32.1           | 33.4    |
| Chinese-LLaMA-Plus-13B  | 28.1  | 33.1       | 35.4            | 35.1   | 33.5           | 33.0    |
| Ziya-LLaMA-13B-Pretrain | 29.0  | 30.7       | 33.8            | 34.4   | 31.9           | 32.1    |
| LLaMA-13B               | 29.2  | 30.8       | 31.6            | 33.0   | 30.5           | 31.2    |
| moss-moon-003-base (16B)| 27.2  | 30.4       | 28.8            | 32.6   | 28.7           | 29.6    |
| **Baichuan-13B-Base**   | **41.7** | **61.1** | **59.8** | **59.0**          | **56.4** | **55.3** |
| **Baichuan-13B-Chat**   | **42.8** | **62.6** | **59.7** | **59.0**          | **56.1** | **55.8** |
> Note: CMMLU is a comprehensive benchmark specifically designed to assess the knowledge and reasoning capabilities of language models in a Chinese context. We adopted the offical [Evaluation Scheme](https://github.com/haonan-li/CMMLU).

# Model Details

| Model Name   | Hidden dim| Layers | Attention Heads | Vocabulary | Total Params       | Training Tokens| Position Embedding                         | Max Length |
|--------------|:---------:|:------:|:---------------:|:----------:|:------------------:|:--------------:|:------------------------------------------:|:----------:|
| Baichuan-7B  | 4,096     | 32     | 32              | 64,000     | 7,000,559,616      | 1.2 Trillion   | [RoPE](https://arxiv.org/abs/2104.09864)   | 4,096      |
| Baichuan-13B | 5,120     | 40     | 40              | 64,000     | 13,264,901,120     | 1.4 Trillion   | [ALiBi](https://arxiv.org/abs/2108.12409)  | 4,096      |

# Inference and Deployment

The model weights, source code, and configuration needed for inference have been published on Hugging Face: [Baichuan-13B-Base](https://huggingface.co/baichuan-inc/Baichuan-13B-Base) and [Baichuan-13B-Chat](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat). The following will use Baichuan-13B-Chat as an example to demonstrate multiple methods of inference. The program will automatically download necessary resources from Hugging Face.

Install requirements before doing inferenceï¼š
```shell
pip install -r requirements.txt
```

## Python Code Inference

```python
>>> import torch
>>> from transformers import AutoModelForCausalLM, AutoTokenizer
>>> from transformers.generation.utils import GenerationConfig
>>> tokenizer = AutoTokenizer.from_pretrained("baichuan-inc/Baichuan-13B-Chat", use_fast=False, trust_remote_code=True)
>>> model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan-13B-Chat", device_map="auto", torch_dtype=torch.float16, trust_remote_code=True)
>>> model.generation_config = GenerationConfig.from_pretrained("baichuan-inc/Baichuan-13B-Chat")
>>> messages = []
>>> messages.append({"role": "user", "content": "What is the second highest mountain in the world?"})
>>> response = model.chat(tokenizer, messages)
>>> print(response)
K2. The tallest one on earth is mount everest, and k2 which located at china-pakistan border ranks as number 2 k2 is also known by its pinyin name "chhogori"
```

> In the above code, the model loading specifies `device_map='auto'`, which will use all available GPUs. If you need to specify the device(s) to use, you can control it in a way similar to `export CUDA_VISIBLE_DEVICES=0,1` (using the 0 and 1 graphics cards).


## Command Line Tool Inference

```shell
python cli_demo.py
```

Demo result as follows:

<p align="center">
    <img src="media/cn-cli.png" width="70%"/>
</p>

## Web Demo Inference

Run the following command using streamlit. It will start a web service locally. You can access it by entering the address provided in the console into your browser.

```shell
streamlit run web_demo.py
```

Demo result as follows:

<p align="center">
    <img src="media/cn-web.gif" width="70%"/>
</p>

## Example outputs of Baichuan-13B-Chat

<details><summary><b>Content Creation</b></summary>

```
ç”¨æˆ·ï¼š
ä½ æ˜¯ä¸€ä¸ªå†…å®¹åˆ›ä½œè€…ï¼Œç¼–å†™ä¸€æ®µå…³äºå¯¹é©¬æ–¯å…‹å’Œæ¯”å°”ç›–èŒ¨è®¿é—®ä¸­å›½è§†è§’çš„çŸ­è§†é¢‘è„šæœ¬

Baichun-13B-Chatï¼š
æ¢ç´¢é©¬æ–¯å…‹ä¸æ¯”å°”Â·ç›–èŒ¨çš„ä¸­å›½ä¹‹æ—…

é•œå¤´1ï¼š(èƒŒæ™¯éŸ³ä¹)ç”»é¢åˆ‡å…¥ï¼Œå±•ç¤ºä¸­å›½çš„ç¹åéƒ½å¸‚æ™¯è±¡ã€‚

æ—ç™½ï¼šæ¬¢è¿æ¥åˆ°æˆ‘ä»¬çš„èŠ‚ç›®ï¼Œä»Šå¤©æˆ‘ä»¬å°†å¸¦æ‚¨ä¸€èµ·äº†è§£ä¸¤ä½ç§‘æŠ€å·¨å¤´â€”â€”åŸƒéš†Â·é©¬æ–¯å…‹å’Œæ¯”å°”Â·ç›–èŒ¨åœ¨ä¸­å›½çš„ä¸€æ¬¡ç‰¹æ®Šè®¿é—®ã€‚

é•œå¤´2ï¼š(èƒŒæ™¯éŸ³ä¹)ç”»é¢åˆ‡æ¢è‡³é©¬æ–¯å…‹å’Œæ¯”å°”Â·ç›–èŒ¨æŠµè¾¾ä¸­å›½çš„æœºåœºã€‚

æ—ç™½ï¼šè¿™æ¬¡è®¿é—®æ˜¯ä»–ä»¬é¦–æ¬¡å…±åŒè¸ä¸Šä¸­å›½çš„åœŸåœ°ï¼Œä»–ä»¬å°†åœ¨è¿™é‡Œå±•å¼€ä¸€åœºå…³äºç§‘æŠ€åˆ›æ–°ã€ç¯ä¿å’Œæ•™è‚²çš„å¯¹è¯ã€‚

é•œå¤´3ï¼š(èƒŒæ™¯éŸ³ä¹)ç”»é¢å±•ç¤ºé©¬æ–¯å…‹å’Œæ¯”å°”Â·ç›–èŒ¨å‚è§‚ä¸€å®¶ç”µåŠ¨æ±½è½¦åˆ¶é€ å‚ã€‚

æ—ç™½ï¼šåœ¨è¿™æ¬¡è®¿é—®ä¸­ï¼Œä»–ä»¬é¦–å…ˆå‚è§‚äº†ä¸€å®¶ç”µåŠ¨æ±½è½¦åˆ¶é€ å‚ï¼Œäº†è§£äº†ä¸­å›½åœ¨æ–°èƒ½æºæ±½è½¦é¢†åŸŸçš„æœ€æ–°å‘å±•ã€‚

é•œå¤´4ï¼š(èƒŒæ™¯éŸ³ä¹)ç”»é¢å±•ç¤ºé©¬æ–¯å…‹å’Œæ¯”å°”Â·ç›–èŒ¨å‚åŠ ä¸€åœºå…³äºäººå·¥æ™ºèƒ½çš„è®ºå›ã€‚

æ—ç™½ï¼šéšåï¼Œä»–ä»¬å‚åŠ äº†å…³äºäººå·¥æ™ºèƒ½çš„ä¸€åœºè®ºå›ï¼Œæ¢è®¨äº†è¿™ä¸€é¢†åŸŸçš„å‘å±•è¶‹åŠ¿åŠå…¶å¯¹æœªæ¥çš„å½±å“ã€‚

é•œå¤´5ï¼š(èƒŒæ™¯éŸ³ä¹)ç”»é¢å±•ç¤ºé©¬æ–¯å…‹å’Œæ¯”å°”Â·ç›–èŒ¨å‚è§‚ä¸€æ‰€å­¦æ ¡ã€‚

æ—ç™½ï¼šåœ¨è¿™æ¬¡è®¿é—®ä¸­ï¼Œä»–ä»¬è¿˜å‚è§‚äº†ä¸€æ‰€å­¦æ ¡ï¼Œäº†è§£äº†ä¸­å›½åœ¨æ•™è‚²é¢†åŸŸçš„åˆ›æ–°å®è·µã€‚

é•œå¤´6ï¼š(èƒŒæ™¯éŸ³ä¹)ç”»é¢å±•ç¤ºé©¬æ–¯å…‹å’Œæ¯”å°”Â·ç›–èŒ¨æ¥å—åª’ä½“é‡‡è®¿ã€‚

æ—ç™½ï¼šæœ€åï¼Œä»–ä»¬åœ¨åª’ä½“é‡‡è®¿ç¯èŠ‚è¡¨ç¤ºï¼Œå¸Œæœ›é€šè¿‡è¿™æ¬¡è®¿é—®ï¼Œèƒ½å¤Ÿä¸ºä¸­ç¾ä¸¤å›½åœ¨ç§‘æŠ€åˆ›æ–°ã€ç¯ä¿å’Œæ•™è‚²ç­‰é¢†åŸŸçš„åˆä½œæä¾›ä¸€äº›å¯ç¤ºã€‚

é•œå¤´7ï¼š(èƒŒæ™¯éŸ³ä¹)ç”»é¢åˆ‡å›ç¹åéƒ½å¸‚æ™¯è±¡ã€‚

æ—ç™½ï¼šè¿™å°±æ˜¯é©¬æ–¯å…‹å’Œæ¯”å°”Â·ç›–èŒ¨çš„ä¸­å›½ä¹‹è¡Œï¼Œä»–ä»¬çš„è®¿é—®ä¸ä»…è®©æˆ‘ä»¬çœ‹åˆ°äº†ä¸­å›½åœ¨ç§‘æŠ€åˆ›æ–°æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¹Ÿä¸ºæˆ‘ä»¬æä¾›äº†å…³äºæœªæ¥å‘å±•çš„æ›´å¤šæ€è€ƒã€‚æ„Ÿè°¢æ”¶çœ‹ï¼Œæˆ‘ä»¬ä¸‹æœŸå†è§ï¼
```

</details>

<details><summary><b>Ads Copywriting</b></summary>
  
```
ç”¨æˆ·ï¼š
è¯·ä»¥é”€å”®å‘˜çš„è§’è‰²ï¼Œå°†æ´—è¡£æ¶²çš„å¹²å‡€ã€æ— æ®‹ç•™ã€æ— å¼‚å‘³ä»¥åŠé™¤èŒç­‰ä¼˜ç‚¹è¿›è¡Œæ¨é”€

Baichun-13B-Chatï¼š
å°Šæ•¬çš„é¡¾å®¢ï¼Œæ‚¨å¥½ï¼æˆ‘æ˜¯ä¸€åæ´—è¡£æ¶²é”€å”®å‘˜ï¼Œéå¸¸é«˜å…´ä¸ºæ‚¨ä»‹ç»æˆ‘ä»¬è¿™æ¬¾äº§å“ã€‚æˆ‘ä»¬çš„æ´—è¡£æ¶²å…·æœ‰ä»¥ä¸‹å‡ ä¸ªæ˜¾è‘—ä¼˜ç‚¹ï¼š

å¹²å‡€ï¼šæˆ‘ä»¬çš„æ´—è¡£æ¶²é‡‡ç”¨äº†å…ˆè¿›çš„æ´—æ¶¤æŠ€æœ¯ï¼Œèƒ½å¤Ÿæ·±å…¥è¡£ç‰©çº¤ç»´ï¼Œæœ‰æ•ˆå»é™¤å„ç§æ±¡æ¸ï¼Œè®©æ‚¨çš„è¡£ç‰©ç„•ç„¶ä¸€æ–°ã€‚åŒæ—¶ï¼Œå®ƒä¸ä¼šå¯¹è¡£ç‰©çš„é¢œè‰²å’Œè´¨åœ°é€ æˆä»»ä½•æŸå®³ï¼Œè®©æ‚¨çš„è¡£ç‰©ä¿æŒåŸæœ‰çš„å“è´¨ã€‚

æ— æ®‹ç•™ï¼šä¸ä¼ ç»Ÿçš„æ´—è¡£ç²‰ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ´—è¡£æ¶²åœ¨æ¸…æ´—è¿‡ç¨‹ä¸­ä¸ä¼šäº§ç”Ÿè¿‡å¤šçš„æ³¡æ²«ï¼Œå› æ­¤æ¸…æ´—åçš„è¡£ç‰©æ›´åŠ æŸ”è½¯ï¼Œä¸ä¼šæœ‰é»è…»çš„æ„Ÿè§‰ã€‚æ­¤å¤–ï¼Œå®ƒçš„å»æ±¡èƒ½åŠ›å¼ºï¼Œèƒ½å½»åº•æ´—å‡€è¡£ç‰©ä¸Šçš„æ±¡æ¸ï¼Œé¿å…äº†ä¼ ç»Ÿæ´—è¡£ç²‰å®¹æ˜“æ®‹ç•™åœ¨è¡£ç‰©ä¸Šçš„é—®é¢˜ã€‚

æ— å¼‚å‘³ï¼šæˆ‘ä»¬çš„æ´—è¡£æ¶²é‡‡ç”¨å¤©ç„¶æ¤ç‰©æå–ç‰©ä½œä¸ºåŸæ–™ï¼Œä¸å«æœ‰å®³åŒ–å­¦ç‰©è´¨ï¼Œå› æ­¤åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ä¸ä¼šäº§ç”Ÿåˆºé¼»çš„æ°”å‘³ã€‚å³ä½¿æ˜¯æ•æ„Ÿè‚Œè‚¤çš„äººç¾¤ä¹Ÿå¯ä»¥æ”¾å¿ƒä½¿ç”¨ï¼Œè®©æ‚¨çš„è¡£ç‰©æ•£å‘å‡ºæ·¡é›…çš„æ¸…é¦™ã€‚

é™¤èŒï¼šä¸ºäº†æ»¡è¶³ç°ä»£äººå¯¹å¥åº·ç”Ÿæ´»çš„éœ€æ±‚ï¼Œæˆ‘ä»¬çš„æ´—è¡£æ¶²ç‰¹åˆ«æ·»åŠ äº†é«˜æ•ˆé™¤èŒæˆåˆ†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ€ç­è¡£ç‰©ä¸Šçš„ç»†èŒï¼Œä¿æŠ¤æ‚¨å’Œå®¶äººçš„èº«ä½“å¥åº·ã€‚æ— è®ºæ˜¯å©´å¹¼å„¿çš„è¡£ç‰©è¿˜æ˜¯è´´èº«å†…è¡£ï¼Œéƒ½å¯ä»¥æ”¾å¿ƒä½¿ç”¨ã€‚

æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„æ´—è¡£æ¶²å‡­å€Ÿå…¶å¹²å‡€ã€æ— æ®‹ç•™ã€æ— å¼‚å‘³ä»¥åŠé™¤èŒç­‰ä¼˜ç‚¹ï¼Œæˆä¸ºäº†å¹¿å¤§æ¶ˆè´¹è€…çš„é¦–é€‰ã€‚å¦‚æœæ‚¨è¿˜æ²¡æœ‰å°è¯•è¿‡æˆ‘ä»¬çš„äº§å“ï¼Œé‚£ä¹ˆç°åœ¨å°±æ˜¯æœ€å¥½çš„æ—¶æœºã€‚ç›¸ä¿¡å®ƒä¸€å®šä¼šç»™æ‚¨å¸¦æ¥å…¨æ–°çš„æ´—è¡£ä½“éªŒï¼
```

```
ç”¨æˆ·ï¼š
å‡†å¤‡ä¸€ä»½ä»¥å® ç‰©é£Ÿå“ä¸ºä¸»é¢˜çš„æŠ¥å‘Šï¼ŒåŒ…æ‹¬æ½œåœ¨æ¶ˆè´¹è€…ï¼Œå¸‚åœºè§„æ¨¡ï¼Œå¸‚åœºæœºé‡ï¼ŒåŠç›ˆåˆ©æ¨¡å¼ç­‰å†…å®¹


Baichun-13B-Chatï¼š

ä¸€ã€å¼•è¨€

éšç€äººä»¬ç”Ÿæ´»æ°´å¹³çš„æé«˜å’Œå¯¹å® ç‰©çš„å…³çˆ±ç¨‹åº¦ä¸æ–­åŠ æ·±ï¼Œå® ç‰©é£Ÿå“è¡Œä¸šå¾—åˆ°äº†å¿«é€Ÿå‘å±•ã€‚æœ¬æŠ¥å‘Šå°†å¯¹å® ç‰©é£Ÿå“è¡Œä¸šçš„æ½œåœ¨æ¶ˆè´¹è€…ã€å¸‚åœºè§„æ¨¡ã€å¸‚åœºæœºé‡ä»¥åŠç›ˆåˆ©æ¨¡å¼ç­‰æ–¹é¢è¿›è¡Œåˆ†æï¼Œä»¥æœŸä¸ºç›¸å…³ä¼ä¸šæä¾›æœ‰ç›Šçš„å‚è€ƒä¿¡æ¯ã€‚

äºŒã€æ½œåœ¨æ¶ˆè´¹è€…

1. å…»å® äººç¾¤ï¼šéšç€å…»å® äººæ•°çš„å¢åŠ ï¼Œå® ç‰©é£Ÿå“çš„éœ€æ±‚ä¹Ÿåœ¨ä¸æ–­å¢é•¿ã€‚æ ¹æ®ç»Ÿè®¡æ•°æ®æ˜¾ç¤ºï¼Œ2019å¹´ä¸­å›½åŸé•‡å…»å® (çŠ¬çŒ«)äººå£æ•°é‡å·²è¾¾7355ä¸‡äººï¼ŒåŒæ¯”å¢é•¿4.6%ã€‚é¢„è®¡åˆ°2023å¹´ï¼Œè¿™ä¸€æ•°å­—å°†è¾¾åˆ°8742ä¸‡äººã€‚

2. æ–°ç”Ÿä»£æ¶ˆè´¹è€…ï¼šæ–°ç”Ÿä»£æ¶ˆè´¹è€…åœ¨æ¶ˆè´¹è§‚å¿µä¸Šæ›´åŠ æ³¨é‡å¥åº·ã€ç¯ä¿å’Œä¸ªæ€§åŒ–ï¼Œè¿™ä½¿å¾—ä»–ä»¬æ›´æ„¿æ„ä¸ºå® ç‰©è´­ä¹°é«˜å“è´¨çš„é£Ÿå“ã€‚æ­¤å¤–ï¼Œä»–ä»¬ä¹Ÿæ›´å€¾å‘äºé€šè¿‡ç½‘ç»œå¹³å°äº†è§£å’Œè´­ä¹°å® ç‰©é£Ÿå“ã€‚

ä¸‰ã€å¸‚åœºè§„æ¨¡

1. æ ¹æ®å›½å®¶ç»Ÿè®¡å±€æ•°æ®ï¼Œ2019å¹´æˆ‘å›½å® ç‰©é£Ÿå“å¸‚åœºè§„æ¨¡è¾¾åˆ°äº†1,020äº¿å…ƒäººæ°‘å¸ï¼ŒåŒæ¯”å¢é•¿çº¦10%ã€‚é¢„è®¡åˆ°2023å¹´ï¼Œå¸‚åœºè§„æ¨¡å°†è¾¾åˆ°1,  500äº¿å…ƒäººæ°‘å¸ã€‚

2. ä»äº§å“ç±»å‹æ¥çœ‹ï¼Œå® ç‰©å¹²ç²®å¸‚åœºå æ¯”æœ€é«˜ï¼Œçº¦å æ€»å¸‚åœºçš„70%;å…¶æ¬¡æ˜¯å® ç‰©æ¹¿ç²®ï¼Œå æ¯”çº¦ä¸º20%;å® ç‰©é›¶é£Ÿå¸‚åœºå æ¯”çº¦ä¸º10%ã€‚

å››ã€å¸‚åœºæœºé‡

1. åŠŸèƒ½æ€§å® ç‰©é£Ÿå“ï¼šéšç€æ¶ˆè´¹è€…å¯¹å¥åº·çš„å…³æ³¨åº¦ä¸æ–­æé«˜ï¼Œå…·æœ‰ç‰¹å®šåŠŸèƒ½çš„å® ç‰©é£Ÿå“å¦‚å¤„æ–¹ç²®ã€å‡è‚¥ç²®ç­‰éœ€æ±‚é€æ¸å¢å¤§ã€‚

2. å¤©ç„¶æœ‰æœºå® ç‰©é£Ÿå“ï¼šè¶Šæ¥è¶Šå¤šçš„æ¶ˆè´¹è€…å€¾å‘äºé€‰æ‹©å¤©ç„¶ã€æ— æ·»åŠ å‰‚çš„å® ç‰©é£Ÿå“ï¼Œè¿™ä¹Ÿä¸ºå¸‚åœºå¸¦æ¥äº†æ–°çš„å•†æœºã€‚

3. å®šåˆ¶åŒ–å® ç‰©é£Ÿå“ï¼šé’ˆå¯¹ä¸åŒå“ç§ã€å¹´é¾„ã€ä½“å‹çš„å® ç‰©ï¼Œæ¨å‡ºå®šåˆ¶åŒ–çš„å® ç‰©é£Ÿå“ï¼Œæ»¡è¶³æ¶ˆè´¹è€…ä¸ªæ€§åŒ–éœ€æ±‚ã€‚

äº”ã€ç›ˆåˆ©æ¨¡å¼

1. ä¼ ç»Ÿé”€å”®æ¸ é“ï¼šé€šè¿‡è¶…å¸‚ã€å® ç‰©åº—ã€ç”µå•†å¹³å°ç­‰ä¼ ç»Ÿæ¸ é“é”€å”®å® ç‰©é£Ÿå“ã€‚

2. ç›´é”€æ¨¡å¼ï¼šé€šè¿‡è‡ªå»ºç½‘ç«™æˆ–ä¸ç¬¬ä¸‰æ–¹å¹³å°åˆä½œï¼Œç›´æ¥å‘æ¶ˆè´¹è€…é”€å”®å® ç‰©é£Ÿå“ã€‚

3. ä¼šå‘˜åˆ¶ï¼šé€šè¿‡è®¾ç«‹ä¼šå‘˜åˆ¶åº¦ï¼Œä¸ºä¼šå‘˜æä¾›ä¸“å±ä¼˜æƒ å’ŒæœåŠ¡ï¼Œå¢åŠ å®¢æˆ·ç²˜æ€§ã€‚

4. å¢å€¼æœåŠ¡ï¼šæä¾›å® ç‰©ç¾å®¹ã€å¯„å…»ã€è®­ç»ƒç­‰é™„åŠ æœåŠ¡ï¼Œå®ç°å¤šå…ƒåŒ–ç›ˆåˆ©ã€‚

å…­ã€ç»“è®º

å® ç‰©é£Ÿå“è¡Œä¸šå…·æœ‰å·¨å¤§çš„å¸‚åœºæ½œåŠ›å’Œå‘å±•ç©ºé—´ã€‚ä¼ä¸šåº”æŠ“ä½å¸‚åœºæœºé‡ï¼Œå…³æ³¨æ¶ˆè´¹è€…éœ€æ±‚å˜åŒ–ï¼Œåˆ›æ–°äº§å“
```

</details>

<details><summary><b>Question Answering</b></summary>

```
ç”¨æˆ·ï¼š
ä¸–ç•Œä¸Šç¬¬äºŒé«˜çš„å±±æ˜¯ä»€ä¹ˆå±±

Baichun-13B-Chatï¼š
ä¹”æˆˆé‡Œå³°(K2) æµ·æ‹”8611ç±³ï¼Œä½äºå–€å–‡æ˜†ä»‘å±±è„‰çš„ä¸­å·´è¾¹å¢ƒä¸Šï¼Œæ˜¯ä¸­å›½ä»¥å¤–ç™»å±±è€…çš„æœ€å¤§è€ƒéªŒï¼Œä¹Ÿæ˜¯ä¸–ç•Œä¸Šæœ€éš¾å¾æœçš„å±±å³°ä¹‹ä¸€ã€‚å®ƒçš„é«˜åº¦ä»…æ¬¡äºç ç©†æœ—ç›å³°ï¼Œä½å±…ä¸–ç•Œç¬¬äºŒã€‚
```

```
ç”¨æˆ·ï¼š
æ„å…´é˜‘çŠæ˜¯ä»€ä¹ˆæ„æ€

Baichun-13B-Chatï¼š
"æ„å…´é˜‘çŠ"æ˜¯ä¸€ä¸ªæ±‰è¯­æˆè¯­ï¼Œå®ƒçš„æ„æ€æ˜¯å½¢å®¹ä¸€ä¸ªäººå…´è‡´ä¸é«˜ï¼Œå…´è¶£å‡å¼±ã€‚è¿™ä¸ªè¯é€šå¸¸ç”¨æ¥æè¿°ä¸€ä¸ªäººåœ¨æŸä¸ªæ´»åŠ¨ä¸­å¤±å»äº†åŸæœ‰çš„çƒ­æƒ…å’Œå…´è¶£ã€‚
```

</details>

<details><summary><b>Language Understanding</b></summary>

```
ç”¨æˆ·ï¼š
å°å¼ ç»™é¢†å¯¼ä¹°äº†ä¸€ä¸ªç¤¼ç‰©è¯´ï¼Œè¯´â€œæ„æ€ä¸€ä¸‹â€ï¼Œé¢†å¯¼è¯´â€œä»€ä¹ˆæ„æ€â€ï¼Œå°å¼ å›ç­”è¯´ï¼šâ€œæ²¡ä»€ä¹ˆæ„æ€ï¼Œå°±æ˜¯éšä¾¿æ„æ€æ„æ€â€ã€‚è¯·ä¾æ¬¡è§£è¯»ä¸€ä¸‹å°å¼ è¯´çš„è¿™å‡ ä¸ªâ€œæ„æ€â€çš„å«ä¹‰ã€‚

Baichun-13B-Chatï¼š
1. ç¬¬ä¸€ä¸ªâ€œæ„æ€â€ï¼šæŒ‡çš„æ˜¯å°å¼ é€ç»™é¢†å¯¼çš„é‚£ä¸ªç¤¼ç‰©ï¼Œè¡¨ç¤ºä»–å·²ç»ä¹°äº†è¿™ä¸ªç¤¼ç‰©å¹¶å‡†å¤‡é€ç»™ä»–ã€‚
2. ç¬¬äºŒä¸ªâ€œæ„æ€â€ï¼šæŒ‡çš„æ˜¯å°å¼ å¯¹é¢†å¯¼çš„ä¸€ç§ç¤¼è²Œå’Œå°Šé‡ï¼Œè¡¨ç¤ºè‡ªå·±å¹¶æ²¡æœ‰å¿˜è®°ä»–çš„èº«ä»½å’Œåœ°ä½ï¼Œåœ¨è´­ä¹°ç¤¼ç‰©æ—¶ä¹Ÿè€ƒè™‘åˆ°äº†è¿™ä¸€ç‚¹ã€‚
3. ç¬¬ä¸‰ä¸ªâ€œæ„æ€â€ï¼šæŒ‡çš„æ˜¯å°å¼ å¯¹äºè¿™æ¬¡èµ é€ç¤¼ç‰©çš„è¡Œä¸ºæœ¬èº«çš„æ€åº¦ï¼Œå³ä¸å¸Œæœ›è¿™ä»¶äº‹æƒ…å¼•èµ·è¿‡å¤šçš„å…³æ³¨æˆ–äº‰è®®ï¼Œåªæ˜¯æƒ³è¡¨è¾¾ä¸€ç§å¿ƒæ„è€Œå·²ã€‚
```

</details>

## Performance of Inference
Baichuan-13B uses ALiBi linear bias technology, which has a smaller computation load compared to Rotary Embedding, significantly improving the inference performance. Compared with standard LLaMA-13B, the average inference speed (tokens/s) has been empirically improved by 31.6%:

| Model       | tokens/s |
|-------------|:--------:|
| LLaMA-13B   | 19.4     |
| Baichuan-13B| 25.4     |

> Enviroment and params used in testing: GPU A100-SXM4-80G, PyTorch 2.0.0+cu117, transformers 4.29.1, batch size = 1, generation length = 2048, Precision fp16, using model Baichuan-13B-Base


## Quantization Deployment

Baichuan-13B supports int8 and int4 quantization, users only need to make a simple two-line change in the inference code to implement it. Please note, if quantization is done to save GPU memory, the original precision model should be loaded onto the CPU before starting quantization. Avoid adding parameters such as `device_map='auto'` or others that could cause the original precision model to be loaded directly onto the GPU when executing `from_pretrained`.

To use int8 quantization:
```python
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan-13B-Chat", torch_dtype=torch.float16, trust_remote_code=True)
model = model.quantize(8).cuda() 
```

Similarly, to use int4 quantization:
```python
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan-13B-Chat", torch_dtype=torch.float16, trust_remote_code=True)
model = model.quantize(4).cuda()
```

Besides, if you don't want to do quantize on the fly, we have [Baichuan-13B-Chat-int8](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat-int8) aviable for int8 quantization of the Chat version:
```python
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan-13B-Chat-int8", torch_dtype=torch.float16, trust_remote_code=True).cuda()
```

The GPU memory usage before and after quantization is as follows:
| Precision   | GPU Mem (GB) |
|-------------|:------------:|
| bf16 / fp16 | 26.0         |
| int8        | 15.8         |
| int4        | 9.7          |

The results on various benchmarks after quantization compared to the original version are as follows:
| Model 5-shot           | C-Eval | MMLU | CMMLU |
|------------------------|:------:|:----:|:-----:|
| Baichuan-13B-Base      | 52.4   | 51.6 | 55.3  |
| Baichuan-13B-Base-int8 | 51.2   | 49.9 | 54.5  |
| Baichuan-13B-Base-int4 | 47.6   | 46.0 | 51.0  |


## CPU Deployment
Baichuan-13B supports CPU inference, but it should be emphasized that the inference speed on CPU will be very slow. Modify the model loading logic as follows:
```python
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan-13B-Chat", torch_dtype=torch.float32, trust_remote_code=True)
```
Loading the entire model approximately requires 60GB of memory.

# Fine-tuning

Developers can fine-tune both Baichuan-13B-Base and Baichuan-13B-Chat for use. Here we have tested the fine-tuning tool [LLaMA Efficient Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) which is compatible with our model, providing demonstrations for both `Full Params Fine-Tuning` and `LoRA Fine-Tuning`.

Before we start, developers should download the project LLaMA Efficient Tuning and [install it's requirements](https://github.com/hiyouga/LLaMA-Efficient-Tuning#getting-started).

The input data are in json files under `data` directory. Use option `--dataset` to specify. For multiple input files, seperate using `,`. The example format and field descriptions of a .json file are as follows:ï¼š
```
[
    {
        "instruction": "What are the three primary colors?",
        "input": "",
        "output": "The three primary colors are red, blue, and yellow."
    },
    ....
]
```
The .json file stores a list, each element of which is a sample. `instruction` represents the user's prompt, `input` is optional. If the developer specifies both `instruction` and `input`, they will be connected with `\n` to represent the user's prompt; `output` represents the expected model output.

Below, we provide demonstration scripts that have been successfully tested in two fine-tuning scenarios.

## Full Params Fine-tuning
We tested under 8 * Nvidia A100 80 GB + deepspeed for full params fine-tuning.

Example of script to start fine-tuning:
```shell
deepspeed --num_gpus=8 src/train_bash.py \
    --stage sft \
    --model_name_or_path baichuan-inc/Baichuan-13B-Base \
    --do_train \
    --dataset alpaca_gpt4_en,alpaca_gpt4_zh \
    --finetuning_type full \
    --output_dir path_to_your_sft_checkpoint \
    --overwrite_cache \
    --per_device_train_batch_size 4 \ 
    --per_device_eval_batch_size 4 \ 
    --gradient_accumulation_steps 8 \ 
    --preprocessing_num_workers 16 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 100 \
    --eval_steps 100 \
    --learning_rate 5e-5 \
    --max_grad_norm 0.5 \
    --num_train_epochs 2.0 \
    --dev_ratio 0.01 \
    --evaluation_strategy steps \
    --load_best_model_at_end \
    --plot_loss \
    --fp16 \
    --deepspeed deepspeed.json
```

Example of deep_speed.json:
```json
{
  "train_micro_batch_size_per_gpu": "auto",
  "zero_allow_untested_optimizer": true,
  "fp16": {
    "enabled": "auto",
    "loss_scale": 0,
    "initial_scale_power": 16, 
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "min_loss_scale": 1
  },  
  "zero_optimization": {
    "stage": 2,
    "allgather_partitions": true,
    "allgather_bucket_size": 5e8,
    "overlap_comm": false,
    "reduce_scatter": true,
    "reduce_bucket_size": 5e8,
    "contiguous_gradients" : true
  }
}
```

## LoRA Fine-tuning
We tested LoRA fine-tuning on a single Nvidia A100 80G GPU.

Example of script to start fine-tuning:
```shell
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --model_name_or_path baichuan-inc/Baichuan-13B-Base \
    --do_train \
    --dataset alpaca_gpt4_en,alpaca_gpt4_zh \
    --finetuning_type lora \
    --lora_rank 8 \ 
    --lora_target W_pack \
    --output_dir path_to_your_sft_checkpoint \
    --overwrite_cache \
    --per_device_train_batch_size 4 \ 
    --per_device_eval_batch_size 4 \ 
    --gradient_accumulation_steps 8 \ 
    --preprocessing_num_workers 16 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 100 \
    --eval_steps 100 \
    --learning_rate 5e-5 \
    --max_grad_norm 0.5 \
    --num_train_epochs 2.0 \
    --dev_ratio 0.01 \
    --evaluation_strategy steps \
    --load_best_model_at_end \
    --plot_loss \
    --fp16
```

For more detailed usage of LLaMA Efficient Tuning, please refer to the instructions on its project homepage.

# Disclaimer

We hereby declare that our team has not developed any applications based on Baichuan-13B model, not on iOS, Android, the web, or any other platform. We strongly call on all users not to use Baichuan-13B model for any activities that harm national / social security or violate the law. Also, we ask users not to use Baichuan-13B model for Internet services that have not undergone appropriate security reviews and filings. We hope that all users can abide by this principle and ensure that the development of technology proceeds in a regulated and legal environment.

We have done our best to ensure the compliance of the data used in the model training process. However, despite our considerable efforts, there may still be some unforeseeable issues due to the complexity of the model and data. Therefore, if any problems arise due to the use of Baichuan-13B open-source model, including but not limited to data security issues, public opinion risks, or any risks and problems brought about by the model being misled, abused, spread or improperly exploited, we will not assume any responsibility.

# Licenses
The use of the source code in this repository complies with the [Apache 2.0](https://github.com/baichuan-inc/Baichuan-13B/blob/main/LICENSE) License. For community use of Baichuan-13B model, see [Community License for Baichuan-13B Model](https://huggingface.co/baichuan-inc/Baichuan-13B-Chat/resolve/main/Community%20License%20for%20Baichuan-13B%20Model.pdf). Baichuan-13B supports commercial use. If you use Baichuan-13B model or its derivatives for commercial purposes, please contact the licensor in the following way to register and apply for written authorization: Contact Email <opensource@baichuan-inc.com>.
